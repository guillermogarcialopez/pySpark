{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ECOSISTEMA SPARK</h1>\n",
    "\n",
    "<h1>Práctica 2: SparkSQL</h1>\n",
    "\n",
    "<h1>TAREA 2</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Autor: Guillermo García López</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color='gray'>Objetivo 1<font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comando para habilitar scrolling horizontal en celdas\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea sparksession para utilidades SparkSQL\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL - Practica 2\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# Se crea SparkContext() para utilidades sobre RDD's\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar la comparación entre tamaños de los formatos .csv y .parquet con el fichero \n",
    "'Phones_acceloremeter.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997515"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos librería os para acceder a la ruta del fichero\n",
    "import os\n",
    "os.path.getsize('./small_data/Phones_accelerometer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los ficheros .csv con la utilidad spark.read.load, y se guarda en formato .parquet, recopilando\n",
    "los tamaños de ambos formatos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos si el directorio con los ficheros .parquet de salida está vacío. Si no, borramos el contenido\n",
    "# antes de guardar los ficheros parquet:\n",
    "import shutil\n",
    "\n",
    "if len(os.listdir('./output_parquet/')) == 0:\n",
    "    pass\n",
    "else:\n",
    "    for file in os.listdir('./output_parquet/'):\n",
    "        shutil.rmtree('./output_parquet/' + file + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPhonesAcc = spark.read.load('./small_data/Phones_accelerometer.csv', format='csv')\n",
    "dfPhonesAcc.write.save('./output_parquet/Phones_accelerometer.parquet', format='parquet')\n",
    "phAcc_csv, phAcc_pqt = os.path.getsize('./small_data/Phones_accelerometer.csv'), \\\n",
    "                       os.path.getsize('./output_parquet/Phones_accelerometer.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPhonesGyr = spark.read.load('./small_data/Phones_gyroscope.csv', format='csv')\n",
    "dfPhonesGyr.write.save('./output_parquet/Phones_gyroscope.parquet', format='parquet')\n",
    "phGyr_csv, phGyr_pqt = os.path.getsize('./small_data/Phones_gyroscope.csv'), \\\n",
    "                       os.path.getsize('./output_parquet/Phones_gyroscope.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWatAcc = spark.read.load('./small_data/Watch_accelerometer.csv', format='csv')\n",
    "dfWatAcc.write.save('./output_parquet/Watch_accelerometer.parquet', format='parquet')\n",
    "WatAcc_csv, WatAcc_pqt = os.path.getsize('./small_data/Watch_accelerometer.csv'), \\\n",
    "                         os.path.getsize('./output_parquet/Watch_accelerometer.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWatGyr = spark.read.load('./small_data/Watch_gyroscope.csv', format='csv')\n",
    "dfWatGyr.write.save('./output_parquet/Watch_gyroscope.parquet', format='parquet')\n",
    "WatGyr_csv, WatGyr_pqt = os.path.getsize('./small_data/Watch_gyroscope.csv'), \\\n",
    "                         os.path.getsize('./output_parquet/Watch_gyroscope.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un rdd con la información de los ficheros .csv, ficheros .parquet y sus tamaños respectivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesizes = [('Phones_accelerometer', phAcc_csv, phAcc_pqt), ('Phones_gyroscope', phGyr_csv, phGyr_pqt),\n",
    "             ('Watch_accelerometer', WatAcc_csv, WatAcc_pqt), ('Watch_gyroscope', WatGyr_csv, WatGyr_pqt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(filesizes)\n",
    "aux = rdd.map(lambda el: Row(file=el[0], csvSize=int(el[1]), parquetSize=int(el[2]))) \n",
    "df = spark.createDataFrame(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y mostramos la tabla con los datos pedidos (tamaños en bytes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----------+\n",
      "|                file|csvSize|parquetSize|\n",
      "+--------------------+-------+-----------+\n",
      "|Phones_accelerometer| 997515|       4096|\n",
      "|    Phones_gyroscope|1100646|       4096|\n",
      "| Watch_accelerometer| 842763|       4096|\n",
      "|     Watch_gyroscope| 894796|       4096|\n",
      "+--------------------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('file', 'csvSize', 'parquetSize').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, los ficheros parquet ocupan muchísimo menos espacio en disco que los csv's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color='gray'>Objetivo 2<font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comparan las ejecuciones de los notebooks de la P1 (ejecución con RDD's), de la tarea 1 de esta misma\n",
    "práctica (dataframes a partir de RDD's), así como de la realización con dataframes a partir de los ficheros parquet\n",
    "y por último de la ejecución con dataframes a partir de los ficheros csv originales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caso 1, P1 (RDD's):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución para rdd's: 3.7700133323669434\n"
     ]
    }
   ],
   "source": [
    "#%run ../P1/P1_2_Spark_GuillermoGarciaLopez.ipynb\n",
    "print('Tiempo de ejecución para rdd\\'s: {0}'.format(3.7700133323669434))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caso 2, P2 Tarea 1 (DataFrames a partir de RDD's) (en este caso no puede ejecutarse desde aquí, \n",
    "  apuntamos el tiempo que tarda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución para dataframes a partir de rdd's: 9.54147219657898\n"
     ]
    }
   ],
   "source": [
    "#%run ./P2_Tarea1_Spark_GuillermoGarciaLopez.ipynb\n",
    "print('Tiempo de ejecución para dataframes a partir de rdd\\'s: {0}'.format(9.54147219657898))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caso 3: DataFrames a partir de ficheros parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "init_time = t.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los ficheros parquet en dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPhAccPqt  = spark.read.parquet('./output_parquet/Phones_accelerometer.parquet/')\n",
    "dfPhGyrPqt  = spark.read.parquet('./output_parquet/Phones_gyroscope.parquet/')\n",
    "dfWatAccPqt = spark.read.parquet('./output_parquet/Watch_accelerometer.parquet/')\n",
    "dfWatGyrPqt = spark.read.parquet('./output_parquet/Watch_gyroscope.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5779860019683838\n"
     ]
    }
   ],
   "source": [
    "print(t.time() - init_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renombramos las columnas ya que parquet no las carga nombradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Función que selecciona las columnas del dataframe que nos interesan y las renombra, además de castearlas al\n",
    "formato conveniente:\n",
    "_c0 -> Index           (int)\n",
    "_c1 -> Arrival_Time    (int)\n",
    "_c2 -> Creation_Time   (int)\n",
    "_c3 -> x               (float)\n",
    "_c4 -> y               (float)\n",
    "_c5 -> z               (float)\n",
    "_c6 -> User            (string)\n",
    "_c7 -> Model           (string)\n",
    "_c8 -> Device          (string)\n",
    "_c9 -> Gt              (string)\n",
    "\n",
    "Input: dataframe\n",
    "Output: dataframe\n",
    "'''\n",
    "def selectAndRename(inDf):\n",
    "    \n",
    "    outDf = inDf.select(inDf[\"_c6\"], inDf[\"_c7\"], inDf[\"_c9\"], inDf[\"_c3\"], inDf[\"_c4\"], inDf[\"_c5\"]) \\\n",
    "                .withColumnRenamed(\"_c6\", \"user\") \\\n",
    "                .withColumnRenamed(\"_c7\", \"model\") \\\n",
    "                .withColumnRenamed(\"_c9\", \"gt\") \\\n",
    "                .withColumnRenamed(\"_c3\", \"x\") \\\n",
    "                .withColumnRenamed(\"_c4\", \"y\") \\\n",
    "                .withColumnRenamed(\"_c5\", \"z\")\n",
    "                \n",
    "    outDf = outDf.selectExpr(\"cast(user as string) user\", \"cast(model as string) model\", \\\n",
    "                             \"cast(gt as string) gt\", \"cast(x as float) x\", \"cast(y as float) y\", \\\n",
    "                             \"cast(z as float) z\")\n",
    "    \n",
    "    return outDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPhAccPqt  = selectAndRename(dfPhAccPqt)\n",
    "dfPhGyrPqt  = selectAndRename(dfPhGyrPqt)\n",
    "dfWatAccPqt = selectAndRename(dfWatAccPqt)\n",
    "dfWatGyrPqt = selectAndRename(dfWatGyrPqt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6503894329071045\n"
     ]
    }
   ],
   "source": [
    "print(t.time() - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y una vez creados los dataframe con el esquema adecuado, realizamos la misma operativa que en P2_Tarea1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = t.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonesStats = dfPhAccPqt.join(dfPhGyrPqt, ['user', 'model', 'gt', 'x', 'y', 'z']  ,'full')\n",
    "watchesStats = dfWatAccPqt.join(dfWatGyrPqt, ['user', 'model', 'gt', 'x', 'y', 'z']  ,'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = phonesStats.unionAll(watchesStats)\n",
    "Stats.createOrReplaceTempView('statsTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28332090377807617\n"
     ]
    }
   ],
   "source": [
    "print(t.time()-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT user, model ,gt ,mean(x), mean(y), mean(z) \\\n",
    "                ,std(x), std(y), std(z), max(x), max(y), max(z), min(x) \\\n",
    "                ,min(y), min(z) \\\n",
    "         FROM statsTable \\\n",
    "         GROUP BY user, model, gt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = t.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.475210666656494\n"
     ]
    }
   ],
   "source": [
    "print(t.time() - c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(t.time() - c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución dataframes desde ficheros parquet: 9.311946392059326\n"
     ]
    }
   ],
   "source": [
    "end_time = t.time()\n",
    "print(\"Tiempo de ejecución dataframes desde ficheros parquet: {0}\".format(end_time - init_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caso 4: DataFrames a partir de ficheros csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = t.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean los dataframes a partir de los csv's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPhAccCsv  = spark.read.csv('./small_data/Phones_accelerometer.csv')\n",
    "dfPhGyrCsv  = spark.read.csv('./small_data/Phones_gyroscope.csv')\n",
    "dfWatAccCsv = spark.read.csv('./small_data/Watch_accelerometer.csv')\n",
    "dfWatGyrCsv  = spark.read.csv('./small_data/Watch_gyroscope.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renombramos y casteamos las columnas del dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPhAccCsv  = selectAndRename(dfPhAccCsv)\n",
    "dfPhGyrCsv  = selectAndRename(dfPhGyrCsv)\n",
    "dfWatAccCsv = selectAndRename(dfWatAccCsv)\n",
    "dfWatGyrCsv = selectAndRename(dfWatGyrCsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y una vez creados los dataframe con el esquema adecuado, realizamos la misma operativa que en P2_Tarea1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonesStats = dfPhAccCsv.join(dfPhGyrCsv, ['user', 'model', 'gt', 'x', 'y', 'z']  ,'full')\n",
    "watchesStats = dfWatAccCsv.join(dfWatGyrCsv, ['user', 'model', 'gt', 'x', 'y', 'z']  ,'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = phonesStats.unionAll(watchesStats)\n",
    "Stats.createOrReplaceTempView('statsTable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y se realiza la query sobre los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+-------------------+-------------------+-------------------+------------------------------+------------------------------+------------------------------+----------+------------+------------+-----------+-----------+----------+\n",
      "|user| model|   gt|             avg(x)|             avg(y)|             avg(z)|stddev_samp(CAST(x AS DOUBLE))|stddev_samp(CAST(y AS DOUBLE))|stddev_samp(CAST(z AS DOUBLE))|    max(x)|      max(y)|      max(z)|     min(x)|     min(y)|    min(z)|\n",
      "+----+------+-----+-------------------+-------------------+-------------------+------------------------------+------------------------------+------------------------------+----------+------------+------------+-----------+-----------+----------+\n",
      "|   a|  gear| null| -4.625535975612993| -1.757118124303339|-0.5569131525539762|             4.644796095861487|            1.7196641033507496|           0.48540930881829963| 0.0561927|-0.018109497|-0.055127434| -9.3583355| -3.5942953|-1.1420342|\n",
      "|   a|nexus4|stand| -3.012455549621582|0.46725270538330077| 4.0070440979003905|             3.017095178875012|           0.49669783926246136|             4.008766102985341| 0.6321869|   1.9472809|    8.638794| -7.0448303|-0.84251404|-0.6001587|\n",
      "|   a|  gear|stand| -4.640251954808826|-1.5871617093284118|-0.5695504666969282|              4.66565617793076|             1.725283385795301|            0.6629781366025101|0.81039995|  0.35446674|   1.1475562| -12.600683|  -11.08276|-2.2625206|\n",
      "|   a|  gear|  sit|-3.7846925775332063|-2.7804111857253284| 1.2819700028308723|             3.816281036409859|             2.750728553136666|            1.3817390266881018|0.39867523|  0.39920786|    3.555988|-10.8229885| -6.9168487|-3.6709096|\n",
      "+----+------+-----+-------------------+-------------------+-------------------+------------------------------+------------------------------+------------------------------+----------+------------+------------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución dataframes desde ficheros csv: 7.144015789031982\n"
     ]
    }
   ],
   "source": [
    "end_time = t.time()\n",
    "print(\"Tiempo de ejecución dataframes desde ficheros csv: {0}\".format(end_time - init_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
